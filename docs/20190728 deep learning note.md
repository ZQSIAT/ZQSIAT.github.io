# 香农熵：
又称为“微分熵”，对整个概率分布中的不确定性总量进行量化。
# KL散度：
用于衡量两个随机变量的概率分布的差异。
主要参考这个博客，他讲解的非常详细，而且还有示例。
<br>[Light on Math Machine Learning: Intuitive Guide to Understanding KL Divergence](http://www.thushv.com/machine-learning/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence/)

<!--supper link, back to home page-->
[^_^]: # (supper link, back to home page)
[back to home page](./..)